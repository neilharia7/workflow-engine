# Select certain nodes for airflow pods.
nodeSelector: {}
affinity: {}
tolerations: []

# Add common labels to all objects and pods defined in this chart.
labels: {}

ingress:
  # enable ingress
  enabled: true

  # Name of tls secret to use on ingress
  tlsSecretName: ~

  # Base domain for ingress vhosts
  baseDomain: ~

  # Enable platform authentication
  auth:
    enabled: false

  annotations:
    # Define the annotation here to configure rewriting rules related to your Load balancer
    #
    # Please note their is a small difference between the way Airflow Web server and Flower handles
    # url_prefix in HTTP requests:
    #  - airflow webserver handles it completely, just let your load balancer to give the HTTP
    #    header like the requested URL (no special configuration neeed)
    #  - Flower wants HTTP header to behave like there was no URL prefix, and but still generates
    #    the right URL in html pages.
    #
    #    Extracted from the Flower documentation:
    #    (https://github.com/mher/flower/blob/master/docs/config.rst#url_prefix)
    #
    #        To access Flower on http://example.com/flower run it with:
    #            flower --url_prefix=flower
    #
    #        Use the following nginx configuration:
    #            server {
    #              listen 80;
    #              server_name example.com;
    #
    #              location /flower/ {
    #                rewrite ^/flower/(.*)$ /$1 break;
    #                proxy_pass http://example.com:5555;
    #                proxy_set_header Host $host;
    #              }
    #            }
    web:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/proxy-body-size: 500m
      ingress.kubernetes.io/proxy-body-size: 500m
      nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
      nginx.ingress.kubernetes.io/proxy-send-timeout: "600"

      # Example for Traefik:
      # traefik.frontend.rule.type: PathPrefix
      # kubernetes.io/ingress.class: traefik
    flower:
      # Example for Traefik:
      # traefik.frontend.rule.type: PathPrefixStrip
      # kubernetes.io/ingress.class: traefik

  host: ""
  path:
    # if web is '/airflow':
    #  - UI will be accessible to '/airflow/admin'
    #  - Healthcheck is at 'http://mycompany.com/airflow/health'
    #  - api is at 'http://mycompany.com/airflow/api'
    web: /
    # if flower is '/airflow/flower':
    #  - api is at 'http://mycompany.com/airflow/flower'

    # TODO change once domain has been setup
    flower: /flower

# Network policy configuration
networkPolicies:
  # Enabled network policies
  enabled: false

# Airflow home directory
# Used for mount paths
airflowHome: "/usr/local/airflow"

env:
  AIRFLOW_HOME: "/usr/local/airflow"

# Default airflow repository
defaultAirflowRepository: 740186269845.dkr.ecr.ap-south-1.amazonaws.com/flowxpert-engine

# Default airflow tag to deploy
# gets changed once CI-CD pipeline kicks
defaultAirflowTag: tagVersion


# credentials to pull image
secrets:
  name: ap-south-1-ecr-registry

# Airflow Images
images:
  airflow:
    # base reposiotry for webserver/scheduler/workers
    repository: 740186269845.dkr.ecr.ap-south-1.amazonaws.com/flowxpert-engine
    # gets changed once CI-CD pipeline kicks
    tag: tagVersion
    # Always or IfNotPresent
    pullPolicy: Always

  flower:
    repository: 740186269845.dkr.ecr.ap-south-1.amazonaws.com/flowxpert-engine
    # gets changed once CI-CD pipeline kicks
    tag: tagVersion
    # Always or IfNotPresent
    pullPolicy: Always

  rabbitmq:
    repository: rabbitmq
    tag: 3-management
    # Always or IfNotPresent
    pullPolicy: IfNotPresent

# Fernet key settings
# You will need to define your frenet key:
# Generate fernet_key with:
#    python -c "from cryptography.fernet import Fernet; FERNET_KEY = Fernet.generate_key().decode(); print(FERNET_KEY)"
# fernet_key: ABCDABCDABCDABCDABCDABCDABCDABCDABCDABCD
fernetKey: ~
fernetKeySecretName: ~

# Airflow Worker Config
workers:
  # Number of airflow celery workers in Deployment / StatefulSet
  replicas: 4

  resources: {}
#    limits:
#      cpu: 800m
#      memory: 1024Mi
#    requests:
#      cpu: 200m
#      memory: 256Mi

  # Grace period for tasks to finish after SIGTERM is sent from kubernetes
  terminationGracePeriodSeconds: 300

  # Apply a HorizontalPodAutoscaler  # TODO future
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilization: 60
    targetMemoryUtilization: 60

# Airflow scheduler settings
scheduler:

  # Same configuration for readiness
  livenessProbe:
    initialDelaySeconds: 20
    timeoutSeconds: 30
    # number of times scheduler is allowed to fail
    # If the scheduler stops heartbeating for 80 seconds (4 * 20), 20 being the period sec. kill the scheduler
    # and let k8s restart it
    failureThreshold: 4  # fail early, fail fast...
    periodSeconds: 20

  resources: {}
#    limits:
#      cpu: 800m
#      memory: 1024Mi
#    requests:
#      cpu: 200m
#      memory: 256Mi

  # This setting tells kubernetes that its ok to evict
  # when it wants to scale a node down.
  safeToEvict: true  # TODO future

# Airflow webserver settings
webserver:
  livenessProbe:
    initialDelaySeconds: 15
    timeoutSeconds: 30
    failureThreshold: 20
    periodSeconds: 5

  readinessProbe:
    initialDelaySeconds: 15
    timeoutSeconds: 30
    failureThreshold: 20
    periodSeconds: 5

  # Number of webservers
  replicas: 1

  resources: {}
#    limits:
#      cpu: 800m
#      memory: 1024Mi
#    requests:
#      cpu: 200m
#      memory: 256Mi

  service:
    type: ClusterIP

# Flower settings
flower:
  # Number of flower
  replicas: 1

  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  # Url prefix for the endpoint of Flower
  # Ex: http://mycompany.com/airflow/flower
  # url_prefix is /airflow/flower

  livenessProbe:
    initialDelaySeconds: 15
    timeoutSeconds: 30
    failureThreshold: 10
    periodSeconds: 5

  readinessProbe:
    initialDelaySeconds: 15
    timeoutSeconds: 30
    failureThreshold: 10
    periodSeconds: 5

  env:
    AIRFLOW_HOME: "/usr/local/airflow"
    # To prevent the error: ValueError: invalid literal for int() with base 10: 'tcp://10.0.0.83:5555'
    FLOWER_PORT: '"5555"'

  service:
    type: ClusterIP

# Rabbitmq Settings
rabbitmq:
  replicas: 2

  resources: {}
    #  limits:
    #   cpu: 100m
    #   memory: 128Mi
    #  requests:
    #   cpu: 100m
    #   memory: 128Mi

  env:
    RABBITMQ_DEFAULT_USER: airflow
    RABBITMQ_DEFAULT_PASS: airflow
    RABBITMQ_DEFAULT_VHOST: airflow

  liveliness:
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold:  5

  service:
    type: ClusterIP
dags:
  # pickle_dag: send DAG using pickle from the scheduler to the worker
  pickle_dag: true
  # Use of git-sync: beware when using git-sync and airflow. If the scheduler reloads a dag in the
  # middle of a dagrun then the dagrun will actually start using the new version of the dag in the
  # middle of execution.
  # This is a known issue with airflow and it means it's unsafe in general to use a git-sync
  # like solution with airflow without:
  # - using explicit locking, ie never pull down a new dag if a dagrun is in progress
  # - make dags immutable, never modify your dag always make a new one
  git_sync_enabled: false
  # url to clone the git repository
  git_repo:
  # branch name
  git_branch: master
  poll_interval_sec: 60
  # print debug logs
  git_sync_debug: false
  # Disable Load examples as soon as you enable git_repo
  load_examples: true

## Configuration values for the postgresql dependency.
## ref: https://github.com/kubernetes/charts/blob/master/stable/postgresql/README.md
##
postgresql:

  ## Use the PostgreSQL chart dependency.
  ## Set to false if bringing your own PostgreSQL.
  ##
  enabled: true

  ## If bringing your own PostgreSQL, the full uri to use
  ## e.g. postgres://airflow:changeme@my-postgres.com:5432/airflow?sslmode=disable
  ##
  # uri:

  ### PostgreSQL User to create.
  ##
  postgresUser: airflow

  ## PostgreSQL Password for the new user.
  ## If not set, a random 10 characters password will be used.
  ##
  postgresPassword: airflow

  ## PostgreSQL Database to create.
  ##
  postgresDatabase: airflow

  ## Persistent Volume Storage configuration.
  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes
  ##
  persistence:
    ## Enable PostgreSQL persistence using Persistent Volume Claims.
    ##
    enabled: true

persistence:
  enabled: false
  ## storageClass: Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  # storageClass: default
  accessMode: ReadWriteOnce
  size: 1Gi

# All ports used by chart
ports:
  flowerUI: 5555
  airflowUI: 8080
  workerLogs: 8793
  rabbitmqNode: 5672
  rabbitmqManagement: 15672

